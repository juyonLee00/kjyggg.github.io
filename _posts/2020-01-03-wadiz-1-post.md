---
title: "python 기초 와디즈 홈페이지 스크래핑하기1-selenium"
date: 2020-1-03 08:26:28 -0400
categories: wadiz crawling scraping 크롤링 스크래핑 
---
###### 안녕하세요 알마덴에서 데이터분석과 솔루션개발을 담당하고 있는 김재영이라고 합니다.
###### 빅데이터팀을 대표로 이렇게 만나게 되어서 정말 반갑습니다. 
###### 첫 글은 크롤링에 대해서 소개해보려고 합니다. 크롤링 기술을 처음으로 소개드리는 이유는 우선 데이터를 수집하는 기술이 데이터 분석의 기초이자 가장 핵심적인 부분이기 때문입니다. 
###### 데이터 분석은 크게 수집, 전처리, 분석, 시각화 4가지로 나뉜다고 볼 수 있는데 이 과정에서 데이터 수집을 위한 방법중 하나로 crawling을 떠올릴 수 있습니다.
###### 크롤링의 장점은 내부 데이터가 없는 개인이 쉽고 빠르게 데이터를 수집할 수 있다는 점입니다. 기업은 내부 데이터를 바탕으로 자신의 상품이나 서비스를 개선하고 분석하지만 개인이 그러한 데이터를 가지기엔 비용과 시간이 충분하지 않습니다. 이 문제에 크롤링은 시간과 비용을 절약해 준다는 점에서 데이터 수집 비용에 대한 좋은 해결책이 될 수 있습니다. 또한 어떠한 페이지든 상관없이 크롤링이 가능하다는 점에서 다양한 정보를 가져올 수 있습니다. 
###### 쉽게 예로들어 설명해보겠습니다. 저희 회사는 상품,서비스에 대한 유저 평가를 보기 위해 와디즈 홈페이지 크롤링을 실시하였습니다. 아래 그림은 필자가 하루 동안 크롤링을 통해 가져온 데이터를 바탕으로 시각화한 결과입니다. 
###### ‘와디즈’ 라는 크라우드 펀딩사이트를 이용했는데요, 크롤링을 통해 해당 게시물 3,000개 이상을 가져와 펀딩 성공 아이템에 대한 정보를 수집하였습니다. 
###### 코드 짜는데 2시간, 크롤링 1시간(총3시간 소요) 걸렸습니다. 3시간이면 나만의 데이터가 쌓이는 겁니다. 참쉽죠^^? 

![3](https://user-images.githubusercontent.com/59075490/71806503-5235ff00-30ac-11ea-9ec5-f9ac08dfc618.png)
###### 만약 크롤링 기술을 이용하지 않고 직접 데이터를 입력했다면 시간과 비용이 만만치 않았겠죠? (아래와 같이 시각화 하는 방법에 대한 포스팅은 추후에 알려드리겠습니다
###### 이제 크롤링이 왜 필요하신지 충분히 이해됐다고 생각하고 크롤링을 위한 기초작업을 시작하겠습니다. 


># selenium 이란?

지금부터 파이썬(python)을 활용하여 크롤링 하는 법을 공부하도록 하겠습니다.

우선 셀레니움에 대해서 공부하도록 할게요. 셀레니움은 웹 테스트 자동화 프레임워크로써, selenium webdriver를 이용하여 다양한 브라우저를 컨트롤 할 수 있습니다. 셀레니움은 구글크롬, 파이어폭스 등의 웹드라이버를 통해 작동해요.
 
BeautifulSoup 같은 다른 웹 수집기도 있지만 이러한 수집기들은 비동기적인 컨텐츠(뒤 늦게 불려오는 컨텐츠)들은 수집하기 매우 어려워요. 

그래서 필자는 셀레니움을 활용하여 비동기 컨텐츠 수집을 시행하려고 해요! 그런데 셀레니움은 속도가 느리다는 점이 있어 실제 코드 구현 시 selenium 사용부분을 최소화 하는 것이 매우 중요해요!

># 2. selenium 설치

셀레니움은 pip를 이용하여 매우 쉽고 간단하게 설치할 수 있어요. 파이썬 터미널창을 켠뒤 아래에 나와있는 코드를 실행시켜 주세요. 
페이지 url을 request를 통해 가져온 뒤 html 본문 내용을 가져오기 위해 Beautifulsoup4를 사용하기 때문에 설치되지 않으신 분은 아래와 같이 설치해주세요.

``` python
pip install selenium
pip install bs4
```


># 3. 크롬 브라우저 설치 및 크롬 웹드라이버 다운로드


 우선 셀레니움은 webdriver를 이용하여 작동하는데 기본적으로 많이 사용하는 크롬 웹드라이버를 사용하여 작동할 예정이에요.

 운영체제 환경마다 세팅하는 방법이 조금씩 다르기 때문에 각각의 운영체제에 대하여 커스터마이징 해야 할 필요가 있어요. 하지만 크롬 웹드라이버를 이용하므로 반드시 크롬 브라우저는 설치되어 있어야 해요! *윈도우, 맥(GUI 가능), Ubuntu, CentOS (GUI 불가능)

앞으로 크롬 웹드라이버를 이용하기 때문에 아래 링크에 들어가서 꼭 크롬 브라우저를 설치해 주세요

 위의 홈페이지 url로 접속해보면 크롬 웹 드라이버를 받을 수 있는 화면이 나와요. Latest Release의 크롬 드라이버링크를 클릭하여 설치해 주세요.
크롬 드라이버 다운로드 :
https://sites.google.com/a/chromium.org/chromedriver/downloads



># 4. selenium 코드 테스트

Ubuntu, CentOs 서버 환경에서는 GUI를 지원하지 않기 때문에 모두 headless 으로 셀레니움 코드를 실행할 필요가 있어요.

셀레니움을 통한 코드 실습 방법은 아래를 참고해 주세요.

```
from selenium import webdriver


options = webdriver.ChromeOptions()

# headless 옵션 설정
options.add_argument('headless')
options.add_argument("no-sandbox")

# 브라우저 윈도우 사이즈
options.add_argument('window-size=1920x1080')

# 사람처럼 보이게 하는 옵션들
options.add_argument("disable-gpu")   # 가속 사용 x
options.add_argument("lang=ko_KR")    # 가짜 플러그인 탑재
options.add_argument('user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36')  # user-agent 이름 설정

# 드라이버 위치 경로 입력
driver = webdriver.Chrome('./chromedriver.exe', chrome_options=options)

driver.get('https://naver.com')
driver.implicitly_wait(3)
driver.get_screenshot_as_file('capture_naver.png')    # 화면캡처

driver.quit() # driver 종료
```
지금까지 우리는 셀레니움을 통한 동적 자료를 가져오기 위한 설치 및 환경 테스트를 시행하였어요. 설치 및 환경 테스트여서 큰 어려움을 없었을 거라 봐요. 혹시 어려운점이 있거나 에러가 발생했다면 댓글로 언제든지 물어봐 주세요!

다음 시간에는 셀레니움을 통해 동적인 URL 정보를 긁어오고 bs4(BeutifulSoup4)를 사용하여 해당 페이지 정보를 가져오는 방법에 대해 배워보도록 하겠습니다.

다음 시간에 봬요!





<!-- Check out the [Jekyll docs][jekyll-docs] for more info on how to get the most out of Jekyll. File all bugs/feature requests at [Jekyll’s GitHub repo][jekyll-gh]. If you have questions, you can ask them on [Jekyll Talk][jekyll-talk].

[jekyll-docs]: https://jekyllrb.com/docs/home
[jekyll-gh]:   https://github.com/jekyll/jekyll
[jekyll-talk]: https://talk.jekyllrb.com/ -->
